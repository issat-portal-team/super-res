{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pickle\n",
    "import defs\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from shutil import unpack_archive,rmtree\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "    file = Path(path)\n",
    "    return file.is_file()\n",
    "\n",
    "def downloadfile(url,output):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(output, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "dataseturl = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\"\n",
    "datasetfilename = \"lfw.tgz\"\n",
    "if not file_exists(datasetfilename):\n",
    "    downloadfile(dataseturl, datasetfilename)\n",
    "else:\n",
    "    print(\"Dataset exists already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rmtree('lfw')\n",
    "except:\n",
    "    pass\n",
    "unpack_archive(datasetfilename, \"lfw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_images = glob.glob('lfw/lfw/**/*.jpg') #returns path of images\n",
    "print(len(face_images)) #contains 13243 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_array():\n",
    "    with open('img_array.pickle','wb') as f:\n",
    "      pickle.dump(img_array, f)\n",
    "    return img_array\n",
    "def load_image_array():\n",
    "    with open('img_array.pickle','rb') as f:\n",
    "        image_array = pickle.load(f)\n",
    "    return image_array\n",
    "\n",
    "if not file_exists('img_array.pickle'):\n",
    "    p = Pool(12)\n",
    "    with Pool(12) as p:\n",
    "        img_array = list(tqdm(p.imap(defs.read, face_images), total=len(face_images),position=0))\n",
    "    save_image_array()\n",
    "else:\n",
    "    img_array = load_image_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = np.array(img_array)\n",
    "#Split test and train data. all_images will be our output images\n",
    "train_x, val_x = train_test_split(all_images, random_state = 32, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will make input images by lowering resolution without changing the size\n",
    "def pixalate_image(image, scale_percent = 40):\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # scale back to original size\n",
    "    width = int(small_image.shape[1] * 100 / scale_percent)\n",
    "    height = int(small_image.shape[0] * 100 / scale_percent)\n",
    "    dim = (width, height)\n",
    "    low_res_image = cv2.resize(small_image, dim, interpolation =  cv2.INTER_AREA)\n",
    "    return low_res_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_px = []\n",
    "for i in range(train_x.shape[0]):\n",
    "    temp = pixalate_image(train_x[i,:,:,:])\n",
    "    train_x_px.append(temp)\n",
    "train_x_px = np.array(train_x_px)   #Distorted images\n",
    "# get low resolution images for the validation set\n",
    "val_x_px = []\n",
    "for i in range(val_x.shape[0]):\n",
    "    temp = pixalate_image(val_x[i,:,:,:])\n",
    "    val_x_px.append(temp)\n",
    "val_x_px = np.array(val_x_px)     #Distorted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autoencoder.evaluate(val_x_px, val_x)\n",
    "print('val_loss, val_accuracy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(val_x_px)\n",
    "n = 4\n",
    "plt.figure(figsize= (20,10))\n",
    "for i in range(n):\n",
    "  ax = plt.subplot(3, n, i+1)\n",
    "  plt.imshow(val_x_px[i+20])\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "  ax = plt.subplot(3, n, i+1+n)\n",
    "  plt.imshow(predictions[i+20])\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
